# -*- coding: utf-8 -*-
"""tfidf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gjTCuLlFmxRHidafBv0QPVy4rGomIK8C
"""

import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from textblob import Word
from sklearn.feature_extraction.text import CountVectorizer

# Sample corpus
documents = ['Machine learning is the study of computer algorithms that improve automatically through experience.\
             Machine learning algorithms build a mathematical model based on sample data, known as training data.\
             The discipline of machine learning employs various approaches to teach computers to accomplish tasks \
             where no fully satisfactory algorithm is available.',
             'Machine learning is closely related to computational statistics, which focuses on making predictions using computers.\
             The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.',
             'Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. \
             It involves computers learning from data provided so that they carry out certain tasks.',
             'Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the "signal"\
             or "feedback" available to the learning system: Supervised, Unsupervised and Reinforcement'
]

pd.set_option('display.max_colwidth', 0)
pd.set_option('display.max_columns', 0)

documents_df=pd.DataFrame(documents,columns=['documents'])

"""Corpus Dataframe"""

documents_df

N=documents_df.shape[0]

import nltk
nltk.download()

nltk.download("stopwords")

documents_df['documents'] = documents_df['documents'].apply(lambda x: " ".join(re.sub('[^a-zA-Z \n]', ' ', x).lower() for x in x.split()))

"""Some preprocessing of text"""

stop = stopwords.words('english')

nltk.download('wordnet')

documents_df['documents'] = documents_df['documents'].apply(lambda x: " ".join(Word(x).lemmatize() for x in x.split() if x not in stop))

"""Documents post preprocessing"""

documents_df

count_vectorizer=CountVectorizer()

count_vectors=count_vectorizer.fit_transform(documents_df['documents'])

count_vectors.toarray()

"""tf-Term Frequency vectors"""

pd.DataFrame(count_vectors.toarray(),columns=[count_vectorizer.get_feature_names()])

# calculating document frequencies from term frequencies
document_frequencies=np.sum(np.where(count_vectors.toarray()>=1,1,0),axis=0)

document_frequencies

"""idf-Inverse document frequencies"""

pd.DataFrame((np.log(N/document_frequencies)).reshape(1,-1),columns=[count_vectorizer.get_feature_names()])

"""tf-idf vectors"""

pd.DataFrame(count_vectors.toarray()*(np.log(N/document_frequencies)),columns=[count_vectorizer.get_feature_names()])

# Sub linear tf scaling
count_vectors_inf=np.where(count_vectors.toarray()==0,float('inf'),count_vectors.toarray())
count_vectors_sublinear=np.where(np.log(count_vectors_inf)==float('inf'),0,1+np.log(count_vectors_inf))

"""wf-idf"""

pd.DataFrame(count_vectors_sublinear*np.log(N/document_frequencies),columns=[count_vectorizer.get_feature_names()])

# Maximum tf normalization
a=0.4
count_vectors_maxnormalized=a+(1-a)*count_vectors.toarray()/np.max(count_vectors.toarray(),axis=1).reshape(-1,1)

"""ntf-idf"""

pd.DataFrame(count_vectors_maxnormalized*np.log(N/document_frequencies),columns=[count_vectorizer.get_feature_names()])