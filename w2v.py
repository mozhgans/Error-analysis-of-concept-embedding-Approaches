# -*- coding: utf-8 -*-
"""W2V.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xs0xaSTtQNho_sGoiCFGM1l-M-Ipr2re
"""

def get_sentences(input_file_pointer):
    while True:
        line = input_file_pointer.readline()
        if not line:
            break

        yield line

import re
def clean_sentence(sentence):
    sentence = sentence.lower().strip()
    sentence = re.sub(r'[^a-z0-9\s]', '', sentence)
    return re.sub(r'\s{2,}', ' ', sentence)

from spacy.lang.en.stop_words import STOP_WORDS
def tokenize(sentence):
    return [token for token in sentence.split() if token not in STOP_WORDS]

sentence = "This is test"

tokenize("This is test."" I am student in canada")

from gensim.models.phrases import Phrases, Phraser
def build_phrases(sentences):
    phrases = Phrases(sentences,
                      min_count=5,
                      threshold=7,
                      progress_per=1000)
    return Phraser(phrases)

build_phrases(sentence)

#phrases_model.save('phrases_model.txt')
#phrases_model= Phraser.load('phrases_model.txt')

def sentence_to_bi_grams(phrases_model, sentence):
    return ' '.join(phrases_model[sentence])

def sentences_to_bi_grams(n_grams, input_file_name, output_file_name):
    with open(input_file_name, 'r') as input_file_pointer:
        with open(output_file_name, 'w+') as out_file:
            for sentence in get_sentences(input_file_pointer):
                cleaned_sentence = clean_sentence(sentence)
                tokenized_sentence = tokenize(cleaned_sentence)
                parsed_sentence = sentence_to_bi_grams(n_grams, tokenized_sentence)
                out_file.write(parsed_sentence + '\n')

!python -m spacy download en_core_web_md

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")
for token in doc:
    print(token.text, token.pos_, token.dep_)

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")
for token in doc:
    print(token.text)

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

#import spacy

#nlp = spacy.load("en_core_web_md")
#tokens = nlp("dog cat banana afskfsd")

#for token in tokens:   print(token.text, token.has_vector, token.vector_norm, token.is_oov)

import spacy
# Load the spacy model that you have installed
#nlp = spacy.load('en_core_web_md')
# process a sentence using the model
#doc = nlp("This is some text that I am processing with Spacy")
# It's that simple - all of the vectors and words are assigned after this point
# Get the vector for 'text':
#doc[3].vector
# Get the mean vector for the entire sentence (useful for sentence classification etc.)
#doc.vector

#from gensim.models import KeyedVectors
# Load vectors directly from the file
#model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)
# Access vectors for specific words with a keyed lookup:
#vector = model['easy']
# see the shape of the vector (300,)
#vector.shape
# Processing sentences is not as simple as with Spacy:
#vectors = [model[x] for x in "This is some text I am processing with Spacy".split(' ')]

import pandas as pd
import gensim
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import xgboost as xgb

main_data = "This is test"
#main_data = pd.read_csv('News_Final.csv')
#main_data.head()

import nltk
nltk.download('punkt')

!python3 -c "import nltk; nltk.download('all')"

# Grab all the titles 
#article_titles = main_data['Title']
# Create a list of strings, one for each title
#titles_list = [title for title in article_titles]

# Collapse the list of strings into a single long string for processing
#big_title_string = ' '.join(titles_list)

from nltk.tokenize import word_tokenize

# Tokenize the string into words
tokens = word_tokenize(main_data)

# Remove non-alphabetic tokens, such as punctuation
words = [word.lower() for word in tokens if word.isalpha()]

# Filter out stopwords
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

words = [word for word in words if not word in stop_words]

# Print first 10 words
words[:10]

!brew install wget

!wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"

import gensim.downloader as api

wv = api.load('word2vec-google-news-300')

vec_king = wv['king']

vec_king

wv.vector_size

economy_vec = wv['economy']
economy_vec[:20] # First 20 components

#from gensim import models

#w = models.KeyedVectors.load_word2vec_format(   '../GoogleNews-vectors-negative300.bin', binary=True)

# Load word2vec model (trained on an enormous Google corpus)
#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) 

# Check dimension of word vectors
#model.vector_size

model = wv

model.vector_size

# Filter the list of vectors to include only those that Word2Vec has a vector for
vector_list = [model[word] for word in words if word in model.vocab]

# Create a list of the words corresponding to these vectors
words_filtered = [word for word in words if word in model.vocab]

# Zip the words together with their vector representations
word_vec_zip = zip(words_filtered, vector_list)

# Cast to a dict so we can turn it into a DataFrame
word_vec_dict = dict(word_vec_zip)
df = pd.DataFrame.from_dict(word_vec_dict, orient='index')
df.head(3)